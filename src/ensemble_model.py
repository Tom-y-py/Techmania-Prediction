"""
Ensemble Model - LightGBM + XGBoost + CatBoost
OPTIMALIZOVAN√Å verze pro MAE < 100
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from scipy.optimize import minimize
import joblib
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# POU≈Ω√çT OPTIMALIZOVAN√ù feature engineering
try:
    from feature_engineering_v3 import create_features, split_data, get_feature_columns
    print("‚úÖ Using OPTIMIZED feature engineering v3")
except ImportError:
    from feature_engineering import create_features, split_data, get_feature_columns
    print("‚ö†Ô∏è Using original feature engineering")


def train_lightgbm(X_train, y_train, X_val, y_val):
    """
    Tr√©nuje LightGBM model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("üå≥ Training LightGBM...")
    print("=" * 60)
    
    # Parametry 
    # Silnƒõj≈°√≠ regularizace + optimalizace pro weather features
    params = {
        'objective': 'regression',
        'metric': 'mae',  # Zmƒõnƒõno z rmse na mae - p≈ô√≠m√° optimalizace MAE!
        'boosting_type': 'gbdt',
        'num_leaves': 25,  # Sn√≠≈æeno - men≈°√≠ overfitting
        'learning_rate': 0.015,  # Ni≈æ≈°√≠ learning rate
        'feature_fraction': 0.7,  # V√≠ce randomizace
        'bagging_fraction': 0.7,
        'bagging_freq': 5,
        'max_depth': 6,  # Sn√≠≈æeno - men≈°√≠ overfitting
        'min_child_samples': 30,  # Zv√Ω≈°eno - v√≠ce robustn√≠
        'reg_alpha': 1.0,  # SILNƒöJ≈†√ç L1 regularizace (z 0.3)
        'reg_lambda': 1.0,  # SILNƒöJ≈†√ç L2 regularizace (z 0.3)
        'min_split_gain': 0.05,  # Vy≈°≈°√≠ - m√©nƒõ splits
        'verbose': -1,
        'random_state': 42
    }
    
    # Dataset pro LightGBM
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    # Tr√©nov√°n√≠
    model = lgb.train(
        params,
        train_data,
        num_boost_round=2000,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )
    
    # Predikce
    train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    val_pred = model.predict(X_val, num_iteration=model.best_iteration)
    
    # Metriky
    print("\n=== LightGBM Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val R¬≤: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.0f}")
    
    return model, val_pred


def train_xgboost(X_train, y_train, X_val, y_val):
    """
    Tr√©nuje XGBoost model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("ÔøΩ Training XGBoost...")
    print("=" * 60)
    
    # Parametry 
    # XGBoost s jinou strategi√≠ + silnƒõj≈°√≠ regularizace
    params = {
        'objective': 'reg:absoluteerror',  # Zmƒõnƒõno na MAE optimalizaci!
        'eval_metric': 'mae',
        'max_depth': 6,  # Sn√≠≈æeno (z 8)
        'learning_rate': 0.01,  # Ni≈æ≈°√≠ (z 0.015)
        'subsample': 0.75,  # Sn√≠≈æeno
        'colsample_bytree': 0.6,
        'colsample_bylevel': 0.75,
        'min_child_weight': 5,  # Zv√Ω≈°eno (z 3)
        'gamma': 0.2,  # Zv√Ω≈°eno
        'reg_alpha': 1.5,  # SILNƒöJ≈†√ç (z 0.2)
        'reg_lambda': 1.5,  # SILNƒöJ≈†√ç (z 0.8)
        'random_state': 43,
        'n_jobs': -1,
        'verbosity': 0
    }
    
    # Dataset pro XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    # Tr√©nov√°n√≠
    evals = [(dtrain, 'train'), (dval, 'valid')]
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=2000,
        evals=evals,
        early_stopping_rounds=100,
        verbose_eval=100
    )
    
    # Predikce
    train_pred = model.predict(dtrain)
    val_pred = model.predict(dval)
    
    # Metriky
    print("\n=== XGBoost Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val R¬≤: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    importance = model.get_score(importance_type='gain')
    feature_importance = pd.DataFrame({
        'feature': list(importance.keys()),
        'importance': list(importance.values())
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.0f}")
    
    return model, val_pred


def train_catboost(X_train, y_train, X_val, y_val):
    """
    Tr√©nuje CatBoost model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("üê± Training CatBoost...")
    print("=" * 60)
    
    # Model 
    # CatBoost s silnƒõj≈°√≠ regularizac√≠
    model = CatBoostRegressor(
        iterations=2000,
        learning_rate=0.02,  # Sn√≠≈æeno (z 0.025)
        depth=6,  # Sn√≠≈æeno (z 8)
        l2_leaf_reg=5,  # Zv√Ω≈°eno (z 3)
        random_strength=0.7,  # Zv√Ω≈°eno
        bagging_temperature=1.0,  # Zv√Ω≈°eno
        rsm=0.65,  # Sn√≠≈æeno
        od_type='Iter',
        od_wait=100,
        random_seed=44,
        verbose=100,
        task_type='CPU',
        bootstrap_type='Bayesian',
        loss_function='MAE'  
    )
    
    # Tr√©nov√°n√≠
    model.fit(
        X_train, y_train,
        eval_set=(X_val, y_val),
        use_best_model=True,
        verbose=100
    )
    
    # Predikce
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)
    
    # Metriky
    print("\n=== CatBoost Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val R¬≤: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    return model, val_pred


def optimize_weights(predictions_dict, y_true, min_weight=0.15):
    """
    Najde optim√°ln√≠ v√°hy pro ensemble pomoc√≠ optimalizace
    S OMEZEN√çM: ka≈æd√Ω model mus√≠ m√≠t minim√°ln√≠ v√°hu pro zaji≈°tƒõn√≠ diverzity
    
    Args:
        predictions_dict: Dict s predikcemi z ka≈æd√©ho modelu
        y_true: Skuteƒçn√© hodnoty
        min_weight: Minim√°ln√≠ v√°ha pro ka≈æd√Ω model (default 0.15 = 15%)
        
    Returns:
        Optim√°ln√≠ v√°hy (numpy array)
    """
    def ensemble_mae(weights):
        ensemble_pred = sum(w * pred for w, pred in zip(weights, predictions_dict.values()))
        return mean_absolute_error(y_true, ensemble_pred)
    
    # Poƒç√°teƒçn√≠ v√°hy (rovnomƒõrn√©)
    n_models = len(predictions_dict)
    initial_weights = [1.0 / n_models] * n_models
    
    # D≈ÆLE≈ΩIT√â OMEZEN√ç: ka≈æd√Ω model mus√≠ m√≠t min_weight a≈æ 1, souƒçet = 1
    # T√≠m zajist√≠me, ≈æe v≈°echny modely p≈ôispƒõj√≠ do ensemble
    bounds = [(min_weight, 1.0)] * n_models
    constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}
    
    # Optimalizace
    result = minimize(
        ensemble_mae,
        initial_weights,
        method='SLSQP',
        bounds=bounds,
        constraints=constraints
    )
    
    return result.x


def create_stacking_ensemble(lgb_pred, xgb_pred, cat_pred, y_true, use_meta_model='ridge'):
    """
    Vytvo≈ô√≠ STACKING ensemble - pou≈æ√≠v√° meta-model k nauƒçen√≠ optim√°ln√≠ kombinace
    Meta-model se uƒç√≠, jak nejl√©pe kombinovat predikce z√°kladn√≠ch model≈Ø
    
    Args:
        lgb_pred: LightGBM predikce
        xgb_pred: XGBoost predikce  
        cat_pred: CatBoost predikce
        y_true: Skuteƒçn√© hodnoty
        use_meta_model: Typ meta-modelu ('ridge', 'lasso', 'rf')
        
    Returns:
        Tuple[meta_model, ensemble_predictions]
    """
    print("\n" + "=" * 60)
    print("üß† Creating STACKING Ensemble with Meta-Model...")
    print("=" * 60)
    
    # Ujistit se, ≈æe v≈°echny predikce maj√≠ stejnou d√©lku
    min_len = min(len(lgb_pred), len(xgb_pred), len(cat_pred))
    lgb_pred = lgb_pred[:min_len]
    xgb_pred = xgb_pred[:min_len]
    cat_pred = cat_pred[:min_len]
    y_true = y_true[:min_len]
    
    # Vytvo≈ôit features pro meta-model = predikce z√°kladn√≠ch model≈Ø
    meta_features = np.column_stack([lgb_pred, xgb_pred, cat_pred])
    
    # Vybrat meta-model
    if use_meta_model == 'ridge':
        # Ridge regression - penalizuje velk√© v√°hy, dob≈ôe generalizuje
        meta_model = Ridge(alpha=1.0, random_state=42)
        print(f"   Meta-model: Ridge Regression (L2 regularization)")
    elif use_meta_model == 'lasso':
        # Lasso - m≈Ø≈æe nastavit nƒõkter√© v√°hy na 0 (feature selection)
        meta_model = Lasso(alpha=0.1, random_state=42)
        print(f"   Meta-model: Lasso Regression (L1 regularization)")
    elif use_meta_model == 'rf':
        # Random Forest - neline√°rn√≠ kombinace, zachyt√≠ komplexn√≠ interakce
        meta_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=3,  # Shallow trees - meta-model by nemƒõl b√Ωt p≈ô√≠li≈° komplexn√≠
            random_state=42,
            n_jobs=-1
        )
        print(f"   Meta-model: Random Forest (non-linear combination)")
    else:
        raise ValueError(f"Nezn√°m√Ω meta-model: {use_meta_model}")
    
    # Tr√©novat meta-model
    meta_model.fit(meta_features, y_true)
    
    # Predikce
    ensemble_pred = meta_model.predict(meta_features)
    
    # Zjistit v√°hy (pro line√°rn√≠ modely)
    if hasattr(meta_model, 'coef_'):
        raw_weights = meta_model.coef_
        # Normalizovat na [0, 1] kter√© seƒçtou do 1
        weights_sum = np.abs(raw_weights).sum()
        if weights_sum > 0:
            weights = np.abs(raw_weights) / weights_sum
        else:
            weights = np.array([1/3, 1/3, 1/3])
        
        print(f"\n=== Meta-Model Learned Weights ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
        print(f"Intercept: {meta_model.intercept_:.2f}")
    else:
        print(f"\n=== Meta-Model Info ===")
        print(f"   Non-linear model - no explicit weights")
        weights = None
    
    # Metriky
    print("\n=== STACKING ENSEMBLE Results ===")
    print(f"Val MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_true, ensemble_pred)):.2f}")
    print(f"Val R¬≤: {r2_score(y_true, ensemble_pred):.4f}")
    
    # Porovn√°n√≠ s jednotliv√Ωmi modely
    print("\n=== Comparison ===")
    print(f"LightGBM MAE: {mean_absolute_error(y_true, lgb_pred):.2f}")
    print(f"XGBoost MAE: {mean_absolute_error(y_true, xgb_pred):.2f}")
    print(f"CatBoost MAE: {mean_absolute_error(y_true, cat_pred):.2f}")
    print(f"Stacking Ensemble MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    
    best_single = min(
        mean_absolute_error(y_true, lgb_pred),
        mean_absolute_error(y_true, xgb_pred),
        mean_absolute_error(y_true, cat_pred)
    )
    ensemble_mae = mean_absolute_error(y_true, ensemble_pred)
    improvement = best_single - ensemble_mae
    improvement_pct = (improvement / best_single) * 100
    
    if improvement > 0:
        print(f"\n‚úÖ Improvement over best single model:")
        print(f"   {improvement:.2f} visitors better ({improvement_pct:.1f}% improvement)")
    elif improvement < 0:
        print(f"\n‚ö†Ô∏è Worse than best single model:")
        print(f"   {abs(improvement):.2f} visitors worse ({abs(improvement_pct):.1f}% degradation)")
    else:
        print(f"\n‚û°Ô∏è Same as best single model")
    
    return meta_model, ensemble_pred, weights


def create_ensemble(lgb_pred, xgb_pred, cat_pred, y_true, optimize=True):
    """
    Vytvo≈ô√≠ ensemble z predikc√≠ v≈°ech model≈Ø
    
    Args:
        lgb_pred: LightGBM predikce
        xgb_pred: XGBoost predikce
        cat_pred: CatBoost predikce
        y_true: Skuteƒçn√© hodnoty
        optimize: Zda optimalizovat v√°hy nebo pou≈æ√≠t defaultn√≠
        
    Returns:
        Tuple[ensemble_predictions, weights]
    """
    print("\n" + "=" * 60)
    print("üéØ Creating Ensemble...")
    print("=" * 60)
    
    # Ujistit se, ≈æe v≈°echny predikce maj√≠ stejnou d√©lku
    min_len = min(len(lgb_pred), len(xgb_pred), len(cat_pred))
    lgb_pred = lgb_pred[:min_len]
    xgb_pred = xgb_pred[:min_len]
    cat_pred = cat_pred[:min_len]
    y_true = y_true[:min_len]
    
    predictions = {
        'lightgbm': lgb_pred,
        'xgboost': xgb_pred,
        'catboost': cat_pred
    }
    
    if optimize:
        # Optimalizuj v√°hy na validaƒçn√≠ch datech
        weights = optimize_weights(predictions, y_true)
        print(f"\n=== Optimized Weights ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
    else:
        # Pokud nen√≠ optimalizace, pou≈æijeme rovnomƒõrn√© v√°hy
        n_models = len(predictions)
        weights = np.array([1.0 / n_models] * n_models)
        print(f"\n=== Equal Weights (No Optimization) ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
    
    # Fin√°ln√≠ predikce
    ensemble_pred = (
        weights[0] * lgb_pred +
        weights[1] * xgb_pred +
        weights[2] * cat_pred
    )
    
    # Metriky
    print("\n=== ENSEMBLE Results ===")
    print(f"Val MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_true, ensemble_pred)):.2f}")
    print(f"Val R¬≤: {r2_score(y_true, ensemble_pred):.4f}")
    
    # Porovn√°n√≠ s jednotliv√Ωmi modely
    print("\n=== Comparison ===")
    print(f"LightGBM MAE: {mean_absolute_error(y_true, lgb_pred):.2f}")
    print(f"XGBoost MAE: {mean_absolute_error(y_true, xgb_pred):.2f}")
    print(f"CatBoost MAE: {mean_absolute_error(y_true, cat_pred):.2f}")
    print(f"Ensemble MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    
    best_single = min(
        mean_absolute_error(y_true, lgb_pred),
        mean_absolute_error(y_true, xgb_pred),
        mean_absolute_error(y_true, cat_pred)
    )
    ensemble_mae = mean_absolute_error(y_true, ensemble_pred)
    improvement = best_single - ensemble_mae
    improvement_pct = (improvement / best_single) * 100
    
    if improvement > 0:
        print(f"\n‚úÖ Improvement over best single model:")
        print(f"   {improvement:.2f} visitors better ({improvement_pct:.1f}% improvement)")
    elif improvement < 0:
        print(f"\n‚ö†Ô∏è Worse than best single model:")
        print(f"   {abs(improvement):.2f} visitors worse ({abs(improvement_pct):.1f}% degradation)")
    else:
        print(f"\n‚û°Ô∏è Same as best single model")
    
    return ensemble_pred, weights


def main():
    """
    Hlavn√≠ pipeline pro ensemble model
    """
    print("\n" + "=" * 70)
    print("üöÄ ENSEMBLE MODEL TRAINING PIPELINE - WITH WEATHER & HOLIDAYS DATA")
    print("=" * 70)
    
    # 1. Naƒç√≠st data S POƒåAS√çM A SV√ÅTKY
    print("\nüìÇ Loading data...")
    
    # Cesta k slouƒçen√Ωm dat≈Øm (n√°v≈°tƒõvnost + poƒças√≠ + sv√°tky + v≈°echny features)
    script_dir = Path(__file__).parent
    data_path = script_dir.parent / 'data' / 'processed' / 'techmania_with_weather_and_holidays.csv'
    
    df = pd.read_csv(data_path)
    print(f"   Loaded {len(df)} records from: {data_path.name}")
    print(f"   Date range: {df['date'].min()} - {df['date'].max()}")
    print(f"   Total columns: {len(df.columns)}")
    
    # Ovƒõ≈ôit, ≈æe m√°me weather data
    weather_cols = ['temperature_mean', 'precipitation', 'weather_code', 'cloud_cover_percent']
    has_weather = all(col in df.columns for col in weather_cols)
    print(f"   Weather data present: {'‚úÖ YES' if has_weather else '‚ùå NO'}")
    
    # Ovƒõ≈ôit, ≈æe m√°me ≈°koln√≠ pr√°zdniny
    holiday_cols = ['is_any_school_break', 'school_break_type', 'is_summer_holiday']
    has_holidays = all(col in df.columns for col in holiday_cols)
    print(f"   School break data present: {'‚úÖ YES' if has_holidays else '‚ùå NO'}")
    
    # 2. Feature engineering
    df = create_features(df)
    
    # 3. Split data
    train, val, test = split_data(df)
    
    # P≈ôipravit X, y
    feature_cols = get_feature_columns(df)
    
    # FILTROVAT pouze numerick√© features (odstranit object dtypes)
    numeric_features = []
    for col in feature_cols:
        if col in df.columns:
            if df[col].dtype in ['int64', 'float64', 'bool', 'int32', 'float32']:
                numeric_features.append(col)
            elif df[col].dtype == 'uint8':  # Nƒõkter√© boolean features
                numeric_features.append(col)
    
    print(f"\nüìã Feature columns ({len(numeric_features)} numeric features):")
    print(f"  {', '.join(numeric_features[:15])}... (+{len(numeric_features)-15} more)")
    
    X_train = train[numeric_features]
    y_train = train['total_visitors']
    X_val = val[numeric_features]
    y_val = val['total_visitors']
    
    # === D≈ÆLE≈ΩIT√â: Normalizace dominantn√≠ho feature 'is_closed' ===
    # is_closed m√° obrovskou importance (1+ miliarda), co≈æ potlaƒçuje vliv poƒças√≠
    # Sn√≠≈æ√≠me jeho v√°hu, aby ostatn√≠ features (poƒças√≠, sv√°tky, atd.) mƒõly vƒõt≈°√≠ vliv
    print("\nüéØ Normalizing 'is_closed' feature to balance importance...")
    if 'is_closed' in X_train.columns:
        # P≈Øvodn√≠ hodnoty: 0 nebo 1
        # Nov√© hodnoty: 0 nebo 0.1 (10x men≈°√≠ vliv)
        # Model se tak mus√≠ v√≠ce spol√©hat na ostatn√≠ features
        original_closed_count_train = X_train['is_closed'].sum()
        original_closed_count_val = X_val['is_closed'].sum()
        
        X_train['is_closed'] = X_train['is_closed'] * 0.1
        X_val['is_closed'] = X_val['is_closed'] * 0.1
        
        print(f"   Train: {original_closed_count_train} zav≈ôen√Ωch dn√≠ ‚Üí v√°ha sn√≠≈æena na 0.1")
        print(f"   Val: {original_closed_count_val} zav≈ôen√Ωch dn√≠ ‚Üí v√°ha sn√≠≈æena na 0.1")
        print(f"   ‚úÖ Weather a ostatn√≠ features nyn√≠ dostanou vƒõt≈°√≠ prostor!")
    else:
        print("   ‚ö†Ô∏è Feature 'is_closed' nebyl nalezen")
    
    # 4. Tr√©novat modely
    lgb_model, lgb_pred = train_lightgbm(X_train, y_train, X_val, y_val)
    
    xgb_model, xgb_pred = train_xgboost(X_train, y_train, X_val, y_val)
    
    cat_model, cat_pred = train_catboost(X_train, y_train, X_val, y_val)
    
    # 5a. Weighted Ensemble (p≈Øvodn√≠ metoda s minim√°ln√≠ v√°hou)
    print("\n" + "=" * 70)
    print("üìä METHOD 1: WEIGHTED ENSEMBLE (optimized weights with minimum)")
    print("=" * 70)
    ensemble_pred_weighted, weights = create_ensemble(
        lgb_pred,
        xgb_pred,
        cat_pred,
        y_val.values,
        optimize=True
    )
    mae_weighted = mean_absolute_error(y_val.values, ensemble_pred_weighted)
    
    # 5b. Stacking Ensemble (meta-model)
    print("\n" + "=" * 70)
    print("üìä METHOD 2: STACKING ENSEMBLE (meta-model learns combination)")
    print("=" * 70)
    meta_model, ensemble_pred_stacking, meta_weights = create_stacking_ensemble(
        lgb_pred,
        xgb_pred,
        cat_pred,
        y_val.values,
        use_meta_model='ridge'  # Ridge je dobr√Ω default
    )
    mae_stacking = mean_absolute_error(y_val.values, ensemble_pred_stacking)
    
    # 5c. Porovnat s nejlep≈°√≠m jednotliv√Ωm modelem
    mae_lgb = mean_absolute_error(y_val.values, lgb_pred)
    mae_xgb = mean_absolute_error(y_val.values, xgb_pred)
    mae_cat = mean_absolute_error(y_val.values, cat_pred)
    best_single_mae = min(mae_lgb, mae_xgb, mae_cat)
    
    print("\n" + "=" * 70)
    print("üèÜ FINAL COMPARISON")
    print("=" * 70)
    print(f"Best Single Model MAE: {best_single_mae:.2f}")
    print(f"Weighted Ensemble MAE: {mae_weighted:.2f}")
    print(f"Stacking Ensemble MAE: {mae_stacking:.2f}")
    
    # Vybrat nejlep≈°√≠ z V≈†ECH mo≈ænost√≠ (vƒçetnƒõ jednotliv√Ωch model≈Ø!)
    candidates = [
        ('best_single', best_single_mae, None),
        ('weighted', mae_weighted, ensemble_pred_weighted),
        ('stacking', mae_stacking, ensemble_pred_stacking)
    ]
    
    winner = min(candidates, key=lambda x: x[1])
    final_ensemble_type = winner[0]
    final_mae = winner[1]
    final_ensemble_pred = winner[2]
    
    if final_ensemble_type == 'stacking':
        print(f"\n‚úÖ Winner: STACKING ENSEMBLE")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   Better than best single by: {best_single_mae - final_mae:.2f} visitors ({((best_single_mae - final_mae) / best_single_mae * 100):.1f}%)")
    elif final_ensemble_type == 'weighted':
        print(f"\n‚úÖ Winner: WEIGHTED ENSEMBLE")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   Better than best single by: {best_single_mae - final_mae:.2f} visitors ({((best_single_mae - final_mae) / best_single_mae * 100):.1f}%)")
    else:
        print(f"\n‚úÖ Winner: BEST SINGLE MODEL (LightGBM)")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   ‚ö†Ô∏è Ensemble methods didn't improve - using single model")
        final_ensemble_type = 'single_lgb'
    
    # 6. Ulo≈æit modely
    print("\nüíæ Saving models...")
    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    os.makedirs(models_dir, exist_ok=True)
    
    joblib.dump(lgb_model, os.path.join(models_dir, 'lightgbm_model.pkl'))
    joblib.dump(xgb_model, os.path.join(models_dir, 'xgboost_model.pkl'))
    joblib.dump(cat_model, os.path.join(models_dir, 'catboost_model.pkl'))
    joblib.dump(weights, os.path.join(models_dir, 'ensemble_weights.pkl'))
    joblib.dump(feature_cols, os.path.join(models_dir, 'feature_columns.pkl'))
    
    # Ulo≈æit informaci o typu ensemble a meta-model (pokud je stacking)
    ensemble_info = {
        'type': final_ensemble_type,
        'mae': final_mae
    }
    if final_ensemble_type == 'stacking':
        joblib.dump(meta_model, os.path.join(models_dir, 'meta_model.pkl'))
        if meta_weights is not None:
            joblib.dump(meta_weights, os.path.join(models_dir, 'meta_weights.pkl'))
        ensemble_info['meta_weights'] = meta_weights
    
    joblib.dump(ensemble_info, os.path.join(models_dir, 'ensemble_info.pkl'))
    
    print("\n‚úÖ Models saved successfully!")
    print("   üìÅ models/lightgbm_model.pkl")
    print("   üìÅ models/xgboost_model.pkl")
    print("   üìÅ models/catboost_model.pkl")
    print("   üìÅ models/ensemble_weights.pkl")
    print("   üìÅ models/feature_columns.pkl")
    print("   üìÅ models/ensemble_info.pkl")
    if final_ensemble_type == 'stacking':
        print("   üìÅ models/meta_model.pkl")
        if meta_weights is not None:
            print("   üìÅ models/meta_weights.pkl")
    
    print("\n" + "=" * 70)
    print("üéâ TRAINING COMPLETE!")
    print("=" * 70)
    print(f"Final Ensemble Type: {final_ensemble_type.upper()}")
    print(f"Final Validation MAE: {final_mae:.2f}")
    
    return {
        'lgb': lgb_model,
        'xgb': xgb_model,
        'cat': cat_model,
        'weights': weights,
        'feature_cols': feature_cols,
        'ensemble_type': final_ensemble_type,
        'meta_model': meta_model if final_ensemble_type == 'stacking' else None,
        'meta_weights': meta_weights if final_ensemble_type == 'stacking' else None
    }


if __name__ == '__main__':
    models = main()

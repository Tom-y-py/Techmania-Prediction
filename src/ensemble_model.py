"""
Ensemble Model - LightGBM + XGBoost + CatBoost
HlavnÃ­ implementace pro trÃ©novÃ¡nÃ­ a kombinaci modelÅ¯
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from scipy.optimize import minimize
import joblib
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from feature_engineering import create_features, split_data, get_feature_columns


def train_lightgbm(X_train, y_train, X_val, y_val):
    """
    TrÃ©nuje LightGBM model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("ðŸŒ³ Training LightGBM...")
    print("=" * 60)
    
    # Parametry - optimalizovÃ¡no pro weather features
    # LightGBM je dobrÃ½ na zachycenÃ­ komplexnÃ­ch interakcÃ­ mezi features
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,  # ZvÃ½Å¡eno - vÃ­ce kapacity pro weather patterns
        'learning_rate': 0.02,  # NiÅ¾Å¡Ã­ learning rate pro lepÅ¡Ã­ generalizaci
        'feature_fraction': 0.75,  # StÅ™ednÃ­ hodnota - dostatek features ale i randomizace
        'bagging_fraction': 0.75,
        'bagging_freq': 5,
        'max_depth': 7,  # ZvÃ½Å¡eno - weather interakce mohou bÃ½t komplexnÃ­
        'min_child_samples': 25,  # BalancovanÃ¡ hodnota
        'reg_alpha': 0.3,  # StÅ™ednÃ­ L1 regularizace
        'reg_lambda': 0.3,  # StÅ™ednÃ­ L2 regularizace
        'min_split_gain': 0.01,  # MinimÃ¡lnÃ­ zisk pro split - redukce noise
        'verbose': -1,
        'random_state': 42
    }
    
    # Dataset pro LightGBM
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    
    # TrÃ©novÃ¡nÃ­
    model = lgb.train(
        params,
        train_data,
        num_boost_round=2000,
        valid_sets=[train_data, val_data],
        valid_names=['train', 'valid'],
        callbacks=[
            lgb.early_stopping(stopping_rounds=100, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )
    
    # Predikce
    train_pred = model.predict(X_train, num_iteration=model.best_iteration)
    val_pred = model.predict(X_val, num_iteration=model.best_iteration)
    
    # Metriky
    print("\n=== LightGBM Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val RÂ²: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importance(importance_type='gain')
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.0f}")
    
    return model, val_pred


def train_xgboost(X_train, y_train, X_val, y_val):
    """
    TrÃ©nuje XGBoost model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("ï¿½ Training XGBoost...")
    print("=" * 60)
    
    # Parametry - XGBoost s jinou strategiÃ­ neÅ¾ LightGBM pro diverzitu
    # XGBoost pouÅ¾Ã­vÃ¡ jinÃ½ algoritmus pro split finding = jinÃ© chyby neÅ¾ LightGBM
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'max_depth': 8,  # HlubÅ¡Ã­ stromy neÅ¾ LightGBM - jinÃ¡ struktura
        'learning_rate': 0.015,  # JeÅ¡tÄ› niÅ¾Å¡Ã­ neÅ¾ LightGBM
        'subsample': 0.8,  # VÃ­ce dat neÅ¾ LightGBM
        'colsample_bytree': 0.6,  # MÃ©nÄ› features - vÄ›tÅ¡Ã­ randomizace
        'colsample_bylevel': 0.8,  # DalÅ¡Ã­ rozmÄ›r randomizace
        'min_child_weight': 3,  # NiÅ¾Å¡Ã­ neÅ¾ LightGBM - jinÃ© chovÃ¡nÃ­
        'gamma': 0.1,  # NiÅ¾Å¡Ã­ neÅ¾ pÅ™ed tÃ­m - umoÅ¾nÃ­ vÃ­ce splits
        'reg_alpha': 0.2,  # NiÅ¾Å¡Ã­ L1 neÅ¾ LightGBM
        'reg_lambda': 0.8,  # VyÅ¡Å¡Ã­ L2 neÅ¾ LightGBM - jinÃ½ typ regularizace
        'random_state': 43,  # JINÃ seed = jinÃ¡ randomizace = diverzita!
        'n_jobs': -1,
        'verbosity': 0
    }
    
    # Dataset pro XGBoost
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    
    # TrÃ©novÃ¡nÃ­
    evals = [(dtrain, 'train'), (dval, 'valid')]
    model = xgb.train(
        params,
        dtrain,
        num_boost_round=2000,
        evals=evals,
        early_stopping_rounds=100,
        verbose_eval=100
    )
    
    # Predikce
    train_pred = model.predict(dtrain)
    val_pred = model.predict(dval)
    
    # Metriky
    print("\n=== XGBoost Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val RÂ²: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    importance = model.get_score(importance_type='gain')
    feature_importance = pd.DataFrame({
        'feature': list(importance.keys()),
        'importance': list(importance.values())
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.0f}")
    
    return model, val_pred


def train_catboost(X_train, y_train, X_val, y_val):
    """
    TrÃ©nuje CatBoost model
    
    Args:
        X_train: Training features
        y_train: Training target
        X_val: Validation features
        y_val: Validation target
        
    Returns:
        Tuple[model, predictions]
    """
    print("\n" + "=" * 60)
    print("ðŸ± Training CatBoost...")
    print("=" * 60)
    
    # Model - CatBoost s native categorical handling
    # CatBoost je speciÃ¡lnÄ› dobrÃ½ pro kategorickÃ© features (weather_code, atd.)
    model = CatBoostRegressor(
        iterations=2000,
        learning_rate=0.025,  # Mezi LightGBM a XGBoost
        depth=8,  # HlubÅ¡Ã­ - CatBoost dobÅ™e zvlÃ¡dÃ¡ hloubku
        l2_leaf_reg=3,  # NiÅ¾Å¡Ã­ neÅ¾ pÅ™ed tÃ­m
        random_strength=0.5,  # VÃ­ce randomizace pro diverzitu
        bagging_temperature=0.8,  # VyÅ¡Å¡Ã­ - agresivnÄ›jÅ¡Ã­ bagging
        rsm=0.7,  # Random subspace method - nÃ¡hodnÃ½ vÃ½bÄ›r features
        od_type='Iter',
        od_wait=100,
        random_seed=44,  # JINÃ seed neÅ¾ ostatnÃ­ modely!
        verbose=100,
        task_type='CPU',
        bootstrap_type='Bayesian'  # JinÃ½ typ bagging neÅ¾ gradient boosting
    )
    
    # TrÃ©novÃ¡nÃ­
    model.fit(
        X_train, y_train,
        eval_set=(X_val, y_val),
        use_best_model=True,
        verbose=100
    )
    
    # Predikce
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)
    
    # Metriky
    print("\n=== CatBoost Results ===")
    print(f"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}")
    print(f"Val MAE: {mean_absolute_error(y_val, val_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_val, val_pred)):.2f}")
    print(f"Val RÂ²: {r2_score(y_val, val_pred):.4f}")
    
    # Feature importance
    print("\n=== Top 10 Important Features ===")
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False).head(10)
    
    for idx, row in feature_importance.iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    return model, val_pred


def optimize_weights(predictions_dict, y_true, min_weight=0.15):
    """
    Najde optimÃ¡lnÃ­ vÃ¡hy pro ensemble pomocÃ­ optimalizace
    S OMEZENÃM: kaÅ¾dÃ½ model musÃ­ mÃ­t minimÃ¡lnÃ­ vÃ¡hu pro zajiÅ¡tÄ›nÃ­ diverzity
    
    Args:
        predictions_dict: Dict s predikcemi z kaÅ¾dÃ©ho modelu
        y_true: SkuteÄnÃ© hodnoty
        min_weight: MinimÃ¡lnÃ­ vÃ¡ha pro kaÅ¾dÃ½ model (default 0.15 = 15%)
        
    Returns:
        OptimÃ¡lnÃ­ vÃ¡hy (numpy array)
    """
    def ensemble_mae(weights):
        ensemble_pred = sum(w * pred for w, pred in zip(weights, predictions_dict.values()))
        return mean_absolute_error(y_true, ensemble_pred)
    
    # PoÄÃ¡teÄnÃ­ vÃ¡hy (rovnomÄ›rnÃ©)
    n_models = len(predictions_dict)
    initial_weights = [1.0 / n_models] * n_models
    
    # DÅ®LEÅ½ITÃ‰ OMEZENÃ: kaÅ¾dÃ½ model musÃ­ mÃ­t min_weight aÅ¾ 1, souÄet = 1
    # TÃ­m zajistÃ­me, Å¾e vÅ¡echny modely pÅ™ispÄ›jÃ­ do ensemble
    bounds = [(min_weight, 1.0)] * n_models
    constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}
    
    # Optimalizace
    result = minimize(
        ensemble_mae,
        initial_weights,
        method='SLSQP',
        bounds=bounds,
        constraints=constraints
    )
    
    return result.x


def create_stacking_ensemble(lgb_pred, xgb_pred, cat_pred, y_true, use_meta_model='ridge'):
    """
    VytvoÅ™Ã­ STACKING ensemble - pouÅ¾Ã­vÃ¡ meta-model k nauÄenÃ­ optimÃ¡lnÃ­ kombinace
    Meta-model se uÄÃ­, jak nejlÃ©pe kombinovat predikce zÃ¡kladnÃ­ch modelÅ¯
    
    Args:
        lgb_pred: LightGBM predikce
        xgb_pred: XGBoost predikce  
        cat_pred: CatBoost predikce
        y_true: SkuteÄnÃ© hodnoty
        use_meta_model: Typ meta-modelu ('ridge', 'lasso', 'rf')
        
    Returns:
        Tuple[meta_model, ensemble_predictions]
    """
    print("\n" + "=" * 60)
    print("ðŸ§  Creating STACKING Ensemble with Meta-Model...")
    print("=" * 60)
    
    # Ujistit se, Å¾e vÅ¡echny predikce majÃ­ stejnou dÃ©lku
    min_len = min(len(lgb_pred), len(xgb_pred), len(cat_pred))
    lgb_pred = lgb_pred[:min_len]
    xgb_pred = xgb_pred[:min_len]
    cat_pred = cat_pred[:min_len]
    y_true = y_true[:min_len]
    
    # VytvoÅ™it features pro meta-model = predikce zÃ¡kladnÃ­ch modelÅ¯
    meta_features = np.column_stack([lgb_pred, xgb_pred, cat_pred])
    
    # Vybrat meta-model
    if use_meta_model == 'ridge':
        # Ridge regression - penalizuje velkÃ© vÃ¡hy, dobÅ™e generalizuje
        meta_model = Ridge(alpha=1.0, random_state=42)
        print(f"   Meta-model: Ridge Regression (L2 regularization)")
    elif use_meta_model == 'lasso':
        # Lasso - mÅ¯Å¾e nastavit nÄ›kterÃ© vÃ¡hy na 0 (feature selection)
        meta_model = Lasso(alpha=0.1, random_state=42)
        print(f"   Meta-model: Lasso Regression (L1 regularization)")
    elif use_meta_model == 'rf':
        # Random Forest - nelineÃ¡rnÃ­ kombinace, zachytÃ­ komplexnÃ­ interakce
        meta_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=3,  # Shallow trees - meta-model by nemÄ›l bÃ½t pÅ™Ã­liÅ¡ komplexnÃ­
            random_state=42,
            n_jobs=-1
        )
        print(f"   Meta-model: Random Forest (non-linear combination)")
    else:
        raise ValueError(f"NeznÃ¡mÃ½ meta-model: {use_meta_model}")
    
    # TrÃ©novat meta-model
    meta_model.fit(meta_features, y_true)
    
    # Predikce
    ensemble_pred = meta_model.predict(meta_features)
    
    # Zjistit vÃ¡hy (pro lineÃ¡rnÃ­ modely)
    if hasattr(meta_model, 'coef_'):
        raw_weights = meta_model.coef_
        # Normalizovat na [0, 1] kterÃ© seÄtou do 1
        weights_sum = np.abs(raw_weights).sum()
        if weights_sum > 0:
            weights = np.abs(raw_weights) / weights_sum
        else:
            weights = np.array([1/3, 1/3, 1/3])
        
        print(f"\n=== Meta-Model Learned Weights ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
        print(f"Intercept: {meta_model.intercept_:.2f}")
    else:
        print(f"\n=== Meta-Model Info ===")
        print(f"   Non-linear model - no explicit weights")
        weights = None
    
    # Metriky
    print("\n=== STACKING ENSEMBLE Results ===")
    print(f"Val MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_true, ensemble_pred)):.2f}")
    print(f"Val RÂ²: {r2_score(y_true, ensemble_pred):.4f}")
    
    # PorovnÃ¡nÃ­ s jednotlivÃ½mi modely
    print("\n=== Comparison ===")
    print(f"LightGBM MAE: {mean_absolute_error(y_true, lgb_pred):.2f}")
    print(f"XGBoost MAE: {mean_absolute_error(y_true, xgb_pred):.2f}")
    print(f"CatBoost MAE: {mean_absolute_error(y_true, cat_pred):.2f}")
    print(f"Stacking Ensemble MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    
    best_single = min(
        mean_absolute_error(y_true, lgb_pred),
        mean_absolute_error(y_true, xgb_pred),
        mean_absolute_error(y_true, cat_pred)
    )
    ensemble_mae = mean_absolute_error(y_true, ensemble_pred)
    improvement = best_single - ensemble_mae
    improvement_pct = (improvement / best_single) * 100
    
    if improvement > 0:
        print(f"\nâœ… Improvement over best single model:")
        print(f"   {improvement:.2f} visitors better ({improvement_pct:.1f}% improvement)")
    elif improvement < 0:
        print(f"\nâš ï¸ Worse than best single model:")
        print(f"   {abs(improvement):.2f} visitors worse ({abs(improvement_pct):.1f}% degradation)")
    else:
        print(f"\nâž¡ï¸ Same as best single model")
    
    return meta_model, ensemble_pred, weights


def create_ensemble(lgb_pred, xgb_pred, cat_pred, y_true, optimize=True):
    """
    VytvoÅ™Ã­ ensemble z predikcÃ­ vÅ¡ech modelÅ¯
    
    Args:
        lgb_pred: LightGBM predikce
        xgb_pred: XGBoost predikce
        cat_pred: CatBoost predikce
        y_true: SkuteÄnÃ© hodnoty
        optimize: Zda optimalizovat vÃ¡hy nebo pouÅ¾Ã­t defaultnÃ­
        
    Returns:
        Tuple[ensemble_predictions, weights]
    """
    print("\n" + "=" * 60)
    print("ðŸŽ¯ Creating Ensemble...")
    print("=" * 60)
    
    # Ujistit se, Å¾e vÅ¡echny predikce majÃ­ stejnou dÃ©lku
    min_len = min(len(lgb_pred), len(xgb_pred), len(cat_pred))
    lgb_pred = lgb_pred[:min_len]
    xgb_pred = xgb_pred[:min_len]
    cat_pred = cat_pred[:min_len]
    y_true = y_true[:min_len]
    
    predictions = {
        'lightgbm': lgb_pred,
        'xgboost': xgb_pred,
        'catboost': cat_pred
    }
    
    if optimize:
        # Optimalizuj vÃ¡hy na validaÄnÃ­ch datech
        weights = optimize_weights(predictions, y_true)
        print(f"\n=== Optimized Weights ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
    else:
        # Pokud nenÃ­ optimalizace, pouÅ¾ijeme rovnomÄ›rnÃ© vÃ¡hy
        n_models = len(predictions)
        weights = np.array([1.0 / n_models] * n_models)
        print(f"\n=== Equal Weights (No Optimization) ===")
        print(f"LightGBM: {weights[0]:.3f}")
        print(f"XGBoost: {weights[1]:.3f}")
        print(f"CatBoost: {weights[2]:.3f}")
    
    # FinÃ¡lnÃ­ predikce
    ensemble_pred = (
        weights[0] * lgb_pred +
        weights[1] * xgb_pred +
        weights[2] * cat_pred
    )
    
    # Metriky
    print("\n=== ENSEMBLE Results ===")
    print(f"Val MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    print(f"Val RMSE: {np.sqrt(mean_squared_error(y_true, ensemble_pred)):.2f}")
    print(f"Val RÂ²: {r2_score(y_true, ensemble_pred):.4f}")
    
    # PorovnÃ¡nÃ­ s jednotlivÃ½mi modely
    print("\n=== Comparison ===")
    print(f"LightGBM MAE: {mean_absolute_error(y_true, lgb_pred):.2f}")
    print(f"XGBoost MAE: {mean_absolute_error(y_true, xgb_pred):.2f}")
    print(f"CatBoost MAE: {mean_absolute_error(y_true, cat_pred):.2f}")
    print(f"Ensemble MAE: {mean_absolute_error(y_true, ensemble_pred):.2f}")
    
    best_single = min(
        mean_absolute_error(y_true, lgb_pred),
        mean_absolute_error(y_true, xgb_pred),
        mean_absolute_error(y_true, cat_pred)
    )
    ensemble_mae = mean_absolute_error(y_true, ensemble_pred)
    improvement = best_single - ensemble_mae
    improvement_pct = (improvement / best_single) * 100
    
    if improvement > 0:
        print(f"\nâœ… Improvement over best single model:")
        print(f"   {improvement:.2f} visitors better ({improvement_pct:.1f}% improvement)")
    elif improvement < 0:
        print(f"\nâš ï¸ Worse than best single model:")
        print(f"   {abs(improvement):.2f} visitors worse ({abs(improvement_pct):.1f}% degradation)")
    else:
        print(f"\nâž¡ï¸ Same as best single model")
    
    return ensemble_pred, weights


def main():
    """
    HlavnÃ­ pipeline pro ensemble model
    """
    print("\n" + "=" * 70)
    print("ðŸš€ ENSEMBLE MODEL TRAINING PIPELINE - WITH WEATHER & HOLIDAYS DATA")
    print("=" * 70)
    
    # 1. NaÄÃ­st data S POÄŒASÃM A SVÃTKY
    print("\nðŸ“‚ Loading data...")
    
    # Cesta k slouÄenÃ½m datÅ¯m (nÃ¡vÅ¡tÄ›vnost + poÄasÃ­ + svÃ¡tky + vÅ¡echny features)
    script_dir = Path(__file__).parent
    data_path = script_dir.parent / 'data' / 'processed' / 'techmania_with_weather_and_holidays.csv'
    
    df = pd.read_csv(data_path)
    print(f"   Loaded {len(df)} records from: {data_path.name}")
    print(f"   Date range: {df['date'].min()} - {df['date'].max()}")
    print(f"   Total columns: {len(df.columns)}")
    
    # OvÄ›Å™it, Å¾e mÃ¡me weather data
    weather_cols = ['temperature_mean', 'precipitation', 'weather_code', 'cloud_cover_percent']
    has_weather = all(col in df.columns for col in weather_cols)
    print(f"   Weather data present: {'âœ… YES' if has_weather else 'âŒ NO'}")
    
    # OvÄ›Å™it, Å¾e mÃ¡me Å¡kolnÃ­ prÃ¡zdniny
    holiday_cols = ['is_any_school_break', 'school_break_type', 'is_summer_holiday']
    has_holidays = all(col in df.columns for col in holiday_cols)
    print(f"   School break data present: {'âœ… YES' if has_holidays else 'âŒ NO'}")
    
    # 2. Feature engineering
    df = create_features(df)
    
    # 3. Split data
    train, val, test = split_data(df)
    
    # PÅ™ipravit X, y
    feature_cols = get_feature_columns(df)
    
    X_train = train[feature_cols]
    y_train = train['total_visitors']
    X_val = val[feature_cols]
    y_val = val['total_visitors']
    
    # === DÅ®LEÅ½ITÃ‰: Normalizace dominantnÃ­ho feature 'is_closed' ===
    # is_closed mÃ¡ obrovskou importance (1+ miliarda), coÅ¾ potlaÄuje vliv poÄasÃ­
    # SnÃ­Å¾Ã­me jeho vÃ¡hu, aby ostatnÃ­ features (poÄasÃ­, svÃ¡tky, atd.) mÄ›ly vÄ›tÅ¡Ã­ vliv
    print("\nðŸŽ¯ Normalizing 'is_closed' feature to balance importance...")
    if 'is_closed' in X_train.columns:
        # PÅ¯vodnÃ­ hodnoty: 0 nebo 1
        # NovÃ© hodnoty: 0 nebo 0.1 (10x menÅ¡Ã­ vliv)
        # Model se tak musÃ­ vÃ­ce spolÃ©hat na ostatnÃ­ features
        original_closed_count_train = X_train['is_closed'].sum()
        original_closed_count_val = X_val['is_closed'].sum()
        
        X_train['is_closed'] = X_train['is_closed'] * 0.1
        X_val['is_closed'] = X_val['is_closed'] * 0.1
        
        print(f"   Train: {original_closed_count_train} zavÅ™enÃ½ch dnÃ­ â†’ vÃ¡ha snÃ­Å¾ena na 0.1")
        print(f"   Val: {original_closed_count_val} zavÅ™enÃ½ch dnÃ­ â†’ vÃ¡ha snÃ­Å¾ena na 0.1")
        print(f"   âœ… Weather a ostatnÃ­ features nynÃ­ dostanou vÄ›tÅ¡Ã­ prostor!")
    else:
        print("   âš ï¸ Feature 'is_closed' nebyl nalezen")
    
    # 4. TrÃ©novat modely
    lgb_model, lgb_pred = train_lightgbm(X_train, y_train, X_val, y_val)
    
    xgb_model, xgb_pred = train_xgboost(X_train, y_train, X_val, y_val)
    
    cat_model, cat_pred = train_catboost(X_train, y_train, X_val, y_val)
    
    # 5a. Weighted Ensemble (pÅ¯vodnÃ­ metoda s minimÃ¡lnÃ­ vÃ¡hou)
    print("\n" + "=" * 70)
    print("ðŸ“Š METHOD 1: WEIGHTED ENSEMBLE (optimized weights with minimum)")
    print("=" * 70)
    ensemble_pred_weighted, weights = create_ensemble(
        lgb_pred,
        xgb_pred,
        cat_pred,
        y_val.values,
        optimize=True
    )
    mae_weighted = mean_absolute_error(y_val.values, ensemble_pred_weighted)
    
    # 5b. Stacking Ensemble (meta-model)
    print("\n" + "=" * 70)
    print("ðŸ“Š METHOD 2: STACKING ENSEMBLE (meta-model learns combination)")
    print("=" * 70)
    meta_model, ensemble_pred_stacking, meta_weights = create_stacking_ensemble(
        lgb_pred,
        xgb_pred,
        cat_pred,
        y_val.values,
        use_meta_model='ridge'  # Ridge je dobrÃ½ default
    )
    mae_stacking = mean_absolute_error(y_val.values, ensemble_pred_stacking)
    
    # 5c. Porovnat s nejlepÅ¡Ã­m jednotlivÃ½m modelem
    mae_lgb = mean_absolute_error(y_val.values, lgb_pred)
    mae_xgb = mean_absolute_error(y_val.values, xgb_pred)
    mae_cat = mean_absolute_error(y_val.values, cat_pred)
    best_single_mae = min(mae_lgb, mae_xgb, mae_cat)
    
    print("\n" + "=" * 70)
    print("ðŸ† FINAL COMPARISON")
    print("=" * 70)
    print(f"Best Single Model MAE: {best_single_mae:.2f}")
    print(f"Weighted Ensemble MAE: {mae_weighted:.2f}")
    print(f"Stacking Ensemble MAE: {mae_stacking:.2f}")
    
    # Vybrat nejlepÅ¡Ã­ z VÅ ECH moÅ¾nostÃ­ (vÄetnÄ› jednotlivÃ½ch modelÅ¯!)
    candidates = [
        ('best_single', best_single_mae, None),
        ('weighted', mae_weighted, ensemble_pred_weighted),
        ('stacking', mae_stacking, ensemble_pred_stacking)
    ]
    
    winner = min(candidates, key=lambda x: x[1])
    final_ensemble_type = winner[0]
    final_mae = winner[1]
    final_ensemble_pred = winner[2]
    
    if final_ensemble_type == 'stacking':
        print(f"\nâœ… Winner: STACKING ENSEMBLE")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   Better than best single by: {best_single_mae - final_mae:.2f} visitors ({((best_single_mae - final_mae) / best_single_mae * 100):.1f}%)")
    elif final_ensemble_type == 'weighted':
        print(f"\nâœ… Winner: WEIGHTED ENSEMBLE")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   Better than best single by: {best_single_mae - final_mae:.2f} visitors ({((best_single_mae - final_mae) / best_single_mae * 100):.1f}%)")
    else:
        print(f"\nâœ… Winner: BEST SINGLE MODEL (LightGBM)")
        print(f"   MAE: {final_mae:.2f}")
        print(f"   âš ï¸ Ensemble methods didn't improve - using single model")
        final_ensemble_type = 'single_lgb'
    
    # 6. UloÅ¾it modely
    print("\nðŸ’¾ Saving models...")
    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    os.makedirs(models_dir, exist_ok=True)
    
    joblib.dump(lgb_model, os.path.join(models_dir, 'lightgbm_model.pkl'))
    joblib.dump(xgb_model, os.path.join(models_dir, 'xgboost_model.pkl'))
    joblib.dump(cat_model, os.path.join(models_dir, 'catboost_model.pkl'))
    joblib.dump(weights, os.path.join(models_dir, 'ensemble_weights.pkl'))
    joblib.dump(feature_cols, os.path.join(models_dir, 'feature_columns.pkl'))
    
    # UloÅ¾it informaci o typu ensemble a meta-model (pokud je stacking)
    ensemble_info = {
        'type': final_ensemble_type,
        'mae': final_mae
    }
    if final_ensemble_type == 'stacking':
        joblib.dump(meta_model, os.path.join(models_dir, 'meta_model.pkl'))
        if meta_weights is not None:
            joblib.dump(meta_weights, os.path.join(models_dir, 'meta_weights.pkl'))
        ensemble_info['meta_weights'] = meta_weights
    
    joblib.dump(ensemble_info, os.path.join(models_dir, 'ensemble_info.pkl'))
    
    print("\nâœ… Models saved successfully!")
    print("   ðŸ“ models/lightgbm_model.pkl")
    print("   ðŸ“ models/xgboost_model.pkl")
    print("   ðŸ“ models/catboost_model.pkl")
    print("   ðŸ“ models/ensemble_weights.pkl")
    print("   ðŸ“ models/feature_columns.pkl")
    print("   ðŸ“ models/ensemble_info.pkl")
    if final_ensemble_type == 'stacking':
        print("   ðŸ“ models/meta_model.pkl")
        if meta_weights is not None:
            print("   ðŸ“ models/meta_weights.pkl")
    
    print("\n" + "=" * 70)
    print("ðŸŽ‰ TRAINING COMPLETE!")
    print("=" * 70)
    print(f"Final Ensemble Type: {final_ensemble_type.upper()}")
    print(f"Final Validation MAE: {final_mae:.2f}")
    
    return {
        'lgb': lgb_model,
        'xgb': xgb_model,
        'cat': cat_model,
        'weights': weights,
        'feature_cols': feature_cols,
        'ensemble_type': final_ensemble_type,
        'meta_model': meta_model if final_ensemble_type == 'stacking' else None,
        'meta_weights': meta_weights if final_ensemble_type == 'stacking' else None
    }


if __name__ == '__main__':
    models = main()
